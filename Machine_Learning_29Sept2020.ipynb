{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP short review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Once when King Krishnadevaraya had gone to survey the jail, two burglars who were prisoners there, asked for his mercy. They told him that they were experts at burglary and could help the king in catching other thieves.The king being a kind ruler asked his guards to release them but with a condition. He told the burglars that he would release them and appoint them as his spies only if they could break into his advisor Tenali Raman’s house and steal valuables from there. The thieves agreed for the challenge.That same night the two thieves went to Tenali Raman’s house and hid behind some bushes. After dinner, when Tenali Raman came out for a stroll, he heard some rustling in the bushes. He at once perceived the existence of thieves in his garden.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Once when King Krishnadevaraya had gone to survey the jail, two burglars who were prisoners there, asked for his mercy. They told him that they were experts at burglary and could help the king in catching other thieves.The king being a kind ruler asked his guards to release them but with a condition. He told the burglars that he would release them and appoint them as his spies only if they could break into his advisor Tenali Raman’s house and steal valuables from there. The thieves agreed for the challenge.That same night the two thieves went to Tenali Raman’s house and hid behind some bushes. After dinner, when Tenali Raman came out for a stroll, he heard some rustling in the bushes. He at once perceived the existence of thieves in his garden.\"\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'once when king krishnadevaraya had gone to survey the jail, two burglars who were prisoners there, asked for his mercy. they told him that they were experts at burglary and could help the king in catching other thieves.the king being a kind ruler asked his guards to release them but with a condition. he told the burglars that he would release them and appoint them as his spies only if they could break into his advisor tenali raman’s house and steal valuables from there. the thieves agreed for the challenge.that same night the two thieves went to tenali raman’s house and hid behind some bushes. after dinner, when tenali raman came out for a stroll, he heard some rustling in the bushes. he at once perceived the existence of thieves in his garden.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower() # for everything in lower case\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['once when king krishnadevaraya had gone to survey the jail, two burglars who were prisoners there, asked for his mercy.', 'they told him that they were experts at burglary and could help the king in catching other thieves.the king being a kind ruler asked his guards to release them but with a condition.', 'he told the burglars that he would release them and appoint them as his spies only if they could break into his advisor tenali raman’s house and steal valuables from there.', 'the thieves agreed for the challenge.that same night the two thieves went to tenali raman’s house and hid behind some bushes.', 'after dinner, when tenali raman came out for a stroll, he heard some rustling in the bushes.', 'he at once perceived the existence of thieves in his garden.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "sent_text = sent_tokenize(text)\n",
    "print(sent_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['once', 'when', 'king', 'krishnadevaraya', 'had', 'gone', 'to', 'survey', 'the', 'jail', ',', 'two', 'burglars', 'who', 'were', 'prisoners', 'there', ',', 'asked', 'for', 'his', 'mercy', '.'], ['they', 'told', 'him', 'that', 'they', 'were', 'experts', 'at', 'burglary', 'and', 'could', 'help', 'the', 'king', 'in', 'catching', 'other', 'thieves.the', 'king', 'being', 'a', 'kind', 'ruler', 'asked', 'his', 'guards', 'to', 'release', 'them', 'but', 'with', 'a', 'condition', '.'], ['he', 'told', 'the', 'burglars', 'that', 'he', 'would', 'release', 'them', 'and', 'appoint', 'them', 'as', 'his', 'spies', 'only', 'if', 'they', 'could', 'break', 'into', 'his', 'advisor', 'tenali', 'raman', '’', 's', 'house', 'and', 'steal', 'valuables', 'from', 'there', '.'], ['the', 'thieves', 'agreed', 'for', 'the', 'challenge.that', 'same', 'night', 'the', 'two', 'thieves', 'went', 'to', 'tenali', 'raman', '’', 's', 'house', 'and', 'hid', 'behind', 'some', 'bushes', '.'], ['after', 'dinner', ',', 'when', 'tenali', 'raman', 'came', 'out', 'for', 'a', 'stroll', ',', 'he', 'heard', 'some', 'rustling', 'in', 'the', 'bushes', '.'], ['he', 'at', 'once', 'perceived', 'the', 'existence', 'of', 'thieves', 'in', 'his', 'garden', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "word_text = []\n",
    "\n",
    "for sent in sent_text:\n",
    "    word_text.append(word_tokenize(sent))\n",
    "    \n",
    "print(word_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'more', 'hadn', 'was', 'him', 'as', 'those', 'he', 'very', 'being', 'no', 'through', 'did', 'hers', 'me', 'after', 'ain', 'above', 'on', 'into', \"you've\", 'who', 'this', \"wasn't\", 'when', 'don', \"haven't\", 'shan', 'same', \"it's\", 'now', \"isn't\", 'them', 'myself', 'what', 'off', 'and', 'his', 'that', 'can', \"that'll\", 'shouldn', 'over', 'until', 'between', 'of', 'has', 'in', 'while', 'down', 'd', 'where', 'doesn', 'does', 'itself', 'too', 'with', 'am', \"couldn't\", 'out', 'some', 'wasn', 'not', 'a', 'up', 'at', 'other', 'all', 'm', 'only', 'you', \"you're\", 'just', 'himself', 'then', \"weren't\", 'wouldn', \"aren't\", 's', 're', 'o', 'needn', 'own', 'haven', 'such', 'nor', 'by', 't', 'yours', \"wouldn't\", 'so', 've', 'than', 'to', \"hasn't\", \"won't\", 'about', 'won', 'whom', 'be', 'their', 'your', 'doing', \"should've\", 'were', \"shan't\", 'are', 'which', 'against', \"hadn't\", 'our', 'ma', 'from', \"you'll\", 'how', 'few', 'any', 'below', 'but', 'herself', \"you'd\", 'should', 'aren', 'been', 'there', 'because', 'under', 'i', 'didn', 'these', 'hasn', 'the', 'my', 'during', 'here', 'why', 'themselves', 'having', 'it', 'couldn', 'y', 'further', \"mustn't\", 'we', 'will', \"mightn't\", \"shouldn't\", 'ourselves', \"needn't\", 'have', 'they', 'most', 'mustn', 'theirs', \"didn't\", \"doesn't\", 'weren', 'both', 'she', 'if', 'do', 'its', 'for', 'yourself', 'ours', 'yourselves', 'before', 'each', 'again', 'had', 'isn', 'once', 'is', \"don't\", 'an', 'or', 'll', 'mightn', 'her', \"she's\"}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords_en=set(stopwords.words('english'))\n",
    "\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['king', 'krishnadevaraya', 'gone', 'survey', 'jail', ',', 'two', 'burglars', 'prisoners', ',', 'asked', 'mercy', '.', 'told', 'experts', 'burglary', 'could', 'help', 'king', 'catching', 'thieves.the', 'king', 'kind', 'ruler', 'asked', 'guards', 'release', 'condition', '.', 'told', 'burglars', 'would', 'release', 'appoint', 'spies', 'could', 'break', 'advisor', 'tenali', 'raman', '’', 'house', 'steal', 'valuables', '.', 'thieves', 'agreed', 'challenge.that', 'night', 'two', 'thieves', 'went', 'tenali', 'raman', '’', 'house', 'hid', 'behind', 'bushes', '.', 'dinner', ',', 'tenali', 'raman', 'came', 'stroll', ',', 'heard', 'rustling', 'bushes', '.', 'perceived', 'existence', 'thieves', 'garden', '.']\n"
     ]
    }
   ],
   "source": [
    "# Eliminate stopwords\n",
    "\n",
    "word_text_filtered = []\n",
    "for w in word_tokenize(text):\n",
    "    if w not in stopwords_en:\n",
    "        word_text_filtered.append(w)\n",
    "print(word_text_filtered)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      "k\n",
      "r\n",
      "i\n",
      "s\n",
      "h\n",
      "n\n",
      "a\n",
      "d\n",
      "e\n",
      "v\n",
      "a\n",
      "r\n",
      "a\n",
      "y\n",
      "a\n",
      "g\n",
      "o\n",
      "n\n",
      "e\n",
      "s\n",
      "u\n",
      "r\n",
      "v\n",
      "e\n",
      "y\n",
      "j\n",
      "a\n",
      "i\n",
      "l\n",
      ",\n",
      "t\n",
      "w\n",
      "o\n",
      "b\n",
      "u\n",
      "r\n",
      "g\n",
      "l\n",
      "a\n",
      "r\n",
      "s\n",
      "p\n",
      "r\n",
      "i\n",
      "s\n",
      "o\n",
      "n\n",
      "e\n",
      "r\n",
      "s\n",
      ",\n",
      "a\n",
      "s\n",
      "k\n",
      "e\n",
      "d\n",
      "m\n",
      "e\n",
      "r\n",
      "c\n",
      "y\n",
      ".\n",
      "t\n",
      "o\n",
      "l\n",
      "d\n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      "s\n",
      "b\n",
      "u\n",
      "r\n",
      "g\n",
      "l\n",
      "a\n",
      "r\n",
      "y\n",
      "c\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      "h\n",
      "e\n",
      "l\n",
      "p\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      "c\n",
      "a\n",
      "t\n",
      "c\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      "t\n",
      "h\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      "s\n",
      ".\n",
      "t\n",
      "h\n",
      "e\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      "k\n",
      "i\n",
      "n\n",
      "d\n",
      "r\n",
      "u\n",
      "l\n",
      "e\n",
      "r\n",
      "a\n",
      "s\n",
      "k\n",
      "e\n",
      "d\n",
      "g\n",
      "u\n",
      "a\n",
      "r\n",
      "d\n",
      "s\n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      "c\n",
      "o\n",
      "n\n",
      "d\n",
      "i\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ".\n",
      "t\n",
      "o\n",
      "l\n",
      "d\n",
      "b\n",
      "u\n",
      "r\n",
      "g\n",
      "l\n",
      "a\n",
      "r\n",
      "s\n",
      "w\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      "a\n",
      "p\n",
      "p\n",
      "o\n",
      "i\n",
      "n\n",
      "t\n",
      "s\n",
      "p\n",
      "i\n",
      "e\n",
      "s\n",
      "c\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      "b\n",
      "r\n",
      "e\n",
      "a\n",
      "k\n",
      "a\n",
      "d\n",
      "v\n",
      "i\n",
      "s\n",
      "o\n",
      "r\n",
      "t\n",
      "e\n",
      "n\n",
      "a\n",
      "l\n",
      "i\n",
      "r\n",
      "a\n",
      "m\n",
      "a\n",
      "n\n",
      "’\n",
      "h\n",
      "o\n",
      "u\n",
      "s\n",
      "e\n",
      "s\n",
      "t\n",
      "e\n",
      "a\n",
      "l\n",
      "v\n",
      "a\n",
      "l\n",
      "u\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      "s\n",
      ".\n",
      "t\n",
      "h\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      "s\n",
      "a\n",
      "g\n",
      "r\n",
      "e\n",
      "e\n",
      "d\n",
      "c\n",
      "h\n",
      "a\n",
      "l\n",
      "l\n",
      "e\n",
      "n\n",
      "g\n",
      "e\n",
      ".\n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      "n\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "t\n",
      "w\n",
      "o\n",
      "t\n",
      "h\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      "s\n",
      "w\n",
      "e\n",
      "n\n",
      "t\n",
      "t\n",
      "e\n",
      "n\n",
      "a\n",
      "l\n",
      "i\n",
      "r\n",
      "a\n",
      "m\n",
      "a\n",
      "n\n",
      "’\n",
      "h\n",
      "o\n",
      "u\n",
      "s\n",
      "e\n",
      "h\n",
      "i\n",
      "d\n",
      "b\n",
      "e\n",
      "h\n",
      "i\n",
      "n\n",
      "d\n",
      "b\n",
      "u\n",
      "s\n",
      "h\n",
      "e\n",
      "s\n",
      ".\n",
      "d\n",
      "i\n",
      "n\n",
      "n\n",
      "e\n",
      "r\n",
      ",\n",
      "t\n",
      "e\n",
      "n\n",
      "a\n",
      "l\n",
      "i\n",
      "r\n",
      "a\n",
      "m\n",
      "a\n",
      "n\n",
      "c\n",
      "a\n",
      "m\n",
      "e\n",
      "s\n",
      "t\n",
      "r\n",
      "o\n",
      "l\n",
      "l\n",
      ",\n",
      "h\n",
      "e\n",
      "a\n",
      "r\n",
      "d\n",
      "r\n",
      "u\n",
      "s\n",
      "t\n",
      "l\n",
      "i\n",
      "n\n",
      "g\n",
      "b\n",
      "u\n",
      "s\n",
      "h\n",
      "e\n",
      "s\n",
      ".\n",
      "p\n",
      "e\n",
      "r\n",
      "c\n",
      "e\n",
      "i\n",
      "v\n",
      "e\n",
      "d\n",
      "e\n",
      "x\n",
      "i\n",
      "s\n",
      "t\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      "t\n",
      "h\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      "s\n",
      "g\n",
      "a\n",
      "r\n",
      "d\n",
      "e\n",
      "n\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for word in word_text_filtered:\n",
    "    for y in word:\n",
    "        print(stemmer.stem(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n",
      "krishnadevaraya\n",
      "gone\n",
      "survey\n",
      "jail\n",
      ",\n",
      "two\n",
      "burglar\n",
      "prison\n",
      ",\n",
      "ask\n",
      "merci\n",
      ".\n",
      "told\n",
      "expert\n",
      "burglari\n",
      "could\n",
      "help\n",
      "king\n",
      "catch\n",
      "thieves.th\n",
      "king\n",
      "kind\n",
      "ruler\n",
      "ask\n",
      "guard\n",
      "releas\n",
      "condit\n",
      ".\n",
      "told\n",
      "burglar\n",
      "would\n",
      "releas\n",
      "appoint\n",
      "spi\n",
      "could\n",
      "break\n",
      "advisor\n",
      "tenali\n",
      "raman\n",
      "’\n",
      "hous\n",
      "steal\n",
      "valuabl\n",
      ".\n",
      "thiev\n",
      "agre\n",
      "challenge.that\n",
      "night\n",
      "two\n",
      "thiev\n",
      "went\n",
      "tenali\n",
      "raman\n",
      "’\n",
      "hous\n",
      "hid\n",
      "behind\n",
      "bush\n",
      ".\n",
      "dinner\n",
      ",\n",
      "tenali\n",
      "raman\n",
      "came\n",
      "stroll\n",
      ",\n",
      "heard\n",
      "rustl\n",
      "bush\n",
      ".\n",
      "perceiv\n",
      "exist\n",
      "thiev\n",
      "garden\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer =  PorterStemmer()\n",
    "for word in word_text_filtered:\n",
    "        print(stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      "k\n",
      "r\n",
      "i\n",
      "s\n",
      "h\n",
      "n\n",
      "a\n",
      "d\n",
      "e\n",
      "v\n",
      "a\n",
      "r\n",
      "a\n",
      "y\n",
      "a\n",
      "g\n",
      "o\n",
      "n\n",
      "e\n",
      "s\n",
      "u\n",
      "r\n",
      "v\n",
      "e\n",
      "y\n",
      "j\n",
      "a\n",
      "i\n",
      "l\n",
      ",\n",
      "t\n",
      "w\n",
      "o\n",
      "b\n",
      "u\n",
      "r\n",
      "g\n",
      "l\n",
      "a\n",
      "r\n",
      "s\n",
      "p\n",
      "r\n",
      "i\n",
      "s\n",
      "o\n",
      "n\n",
      "e\n",
      "r\n",
      "s\n",
      ",\n",
      "a\n",
      "s\n",
      "k\n",
      "e\n",
      "d\n",
      "m\n",
      "e\n",
      "r\n",
      "c\n",
      "y\n",
      ".\n",
      "t\n",
      "o\n",
      "l\n",
      "d\n",
      "e\n",
      "x\n",
      "p\n",
      "e\n",
      "r\n",
      "t\n",
      "s\n",
      "b\n",
      "u\n",
      "r\n",
      "g\n",
      "l\n",
      "a\n",
      "r\n",
      "y\n",
      "c\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      "h\n",
      "e\n",
      "l\n",
      "p\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      "c\n",
      "a\n",
      "t\n",
      "c\n",
      "h\n",
      "i\n",
      "n\n",
      "g\n",
      "t\n",
      "h\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      "s\n",
      ".\n",
      "t\n",
      "h\n",
      "e\n",
      "k\n",
      "i\n",
      "n\n",
      "g\n",
      "k\n",
      "i\n",
      "n\n",
      "d\n",
      "r\n",
      "u\n",
      "l\n",
      "e\n",
      "r\n",
      "a\n",
      "s\n",
      "k\n",
      "e\n",
      "d\n",
      "g\n",
      "u\n",
      "a\n",
      "r\n",
      "d\n",
      "s\n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      "c\n",
      "o\n",
      "n\n",
      "d\n",
      "i\n",
      "t\n",
      "i\n",
      "o\n",
      "n\n",
      ".\n",
      "t\n",
      "o\n",
      "l\n",
      "d\n",
      "b\n",
      "u\n",
      "r\n",
      "g\n",
      "l\n",
      "a\n",
      "r\n",
      "s\n",
      "w\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      "r\n",
      "e\n",
      "l\n",
      "e\n",
      "a\n",
      "s\n",
      "e\n",
      "a\n",
      "p\n",
      "p\n",
      "o\n",
      "i\n",
      "n\n",
      "t\n",
      "s\n",
      "p\n",
      "i\n",
      "e\n",
      "s\n",
      "c\n",
      "o\n",
      "u\n",
      "l\n",
      "d\n",
      "b\n",
      "r\n",
      "e\n",
      "a\n",
      "k\n",
      "a\n",
      "d\n",
      "v\n",
      "i\n",
      "s\n",
      "o\n",
      "r\n",
      "t\n",
      "e\n",
      "n\n",
      "a\n",
      "l\n",
      "i\n",
      "r\n",
      "a\n",
      "m\n",
      "a\n",
      "n\n",
      "’\n",
      "h\n",
      "o\n",
      "u\n",
      "s\n",
      "e\n",
      "s\n",
      "t\n",
      "e\n",
      "a\n",
      "l\n",
      "v\n",
      "a\n",
      "l\n",
      "u\n",
      "a\n",
      "b\n",
      "l\n",
      "e\n",
      "s\n",
      ".\n",
      "t\n",
      "h\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      "s\n",
      "a\n",
      "g\n",
      "r\n",
      "e\n",
      "e\n",
      "d\n",
      "c\n",
      "h\n",
      "a\n",
      "l\n",
      "l\n",
      "e\n",
      "n\n",
      "g\n",
      "e\n",
      ".\n",
      "t\n",
      "h\n",
      "a\n",
      "t\n",
      "n\n",
      "i\n",
      "g\n",
      "h\n",
      "t\n",
      "t\n",
      "w\n",
      "o\n",
      "t\n",
      "h\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      "s\n",
      "w\n",
      "e\n",
      "n\n",
      "t\n",
      "t\n",
      "e\n",
      "n\n",
      "a\n",
      "l\n",
      "i\n",
      "r\n",
      "a\n",
      "m\n",
      "a\n",
      "n\n",
      "’\n",
      "h\n",
      "o\n",
      "u\n",
      "s\n",
      "e\n",
      "h\n",
      "i\n",
      "d\n",
      "b\n",
      "e\n",
      "h\n",
      "i\n",
      "n\n",
      "d\n",
      "b\n",
      "u\n",
      "s\n",
      "h\n",
      "e\n",
      "s\n",
      ".\n",
      "d\n",
      "i\n",
      "n\n",
      "n\n",
      "e\n",
      "r\n",
      ",\n",
      "t\n",
      "e\n",
      "n\n",
      "a\n",
      "l\n",
      "i\n",
      "r\n",
      "a\n",
      "m\n",
      "a\n",
      "n\n",
      "c\n",
      "a\n",
      "m\n",
      "e\n",
      "s\n",
      "t\n",
      "r\n",
      "o\n",
      "l\n",
      "l\n",
      ",\n",
      "h\n",
      "e\n",
      "a\n",
      "r\n",
      "d\n",
      "r\n",
      "u\n",
      "s\n",
      "t\n",
      "l\n",
      "i\n",
      "n\n",
      "g\n",
      "b\n",
      "u\n",
      "s\n",
      "h\n",
      "e\n",
      "s\n",
      ".\n",
      "p\n",
      "e\n",
      "r\n",
      "c\n",
      "e\n",
      "i\n",
      "v\n",
      "e\n",
      "d\n",
      "e\n",
      "x\n",
      "i\n",
      "s\n",
      "t\n",
      "e\n",
      "n\n",
      "c\n",
      "e\n",
      "t\n",
      "h\n",
      "i\n",
      "e\n",
      "v\n",
      "e\n",
      "s\n",
      "g\n",
      "a\n",
      "r\n",
      "d\n",
      "e\n",
      "n\n",
      ".\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer =WordNetLemmatizer()\n",
    "\n",
    "for word in word_text_filtered:\n",
    "    for y in word:\n",
    "        print(lemmatizer.lemmatize(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "king\n",
      "krishnadevaraya\n",
      "gone\n",
      "survey\n",
      "jail\n",
      ",\n",
      "two\n",
      "burglar\n",
      "prisoner\n",
      ",\n",
      "asked\n",
      "mercy\n",
      ".\n",
      "told\n",
      "expert\n",
      "burglary\n",
      "could\n",
      "help\n",
      "king\n",
      "catching\n",
      "thieves.the\n",
      "king\n",
      "kind\n",
      "ruler\n",
      "asked\n",
      "guard\n",
      "release\n",
      "condition\n",
      ".\n",
      "told\n",
      "burglar\n",
      "would\n",
      "release\n",
      "appoint\n",
      "spy\n",
      "could\n",
      "break\n",
      "advisor\n",
      "tenali\n",
      "raman\n",
      "’\n",
      "house\n",
      "steal\n",
      "valuable\n",
      ".\n",
      "thief\n",
      "agreed\n",
      "challenge.that\n",
      "night\n",
      "two\n",
      "thief\n",
      "went\n",
      "tenali\n",
      "raman\n",
      "’\n",
      "house\n",
      "hid\n",
      "behind\n",
      "bush\n",
      ".\n",
      "dinner\n",
      ",\n",
      "tenali\n",
      "raman\n",
      "came\n",
      "stroll\n",
      ",\n",
      "heard\n",
      "rustling\n",
      "bush\n",
      ".\n",
      "perceived\n",
      "existence\n",
      "thief\n",
      "garden\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lammatizer =  WordNetLemmatizer()\n",
    "for word in word_text_filtered:\n",
    "        print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('once', 'RB'), ('when', 'WRB'), ('king', 'VBG'), ('krishnadevaraya', 'NN'), ('had', 'VBD'), ('gone', 'VBN'), ('to', 'TO'), ('survey', 'NN'), ('the', 'DT'), ('jail', 'NN'), (',', ','), ('two', 'CD'), ('burglars', 'NNS'), ('who', 'WP'), ('were', 'VBD'), ('prisoners', 'NNS'), ('there', 'RB'), (',', ','), ('asked', 'VBD'), ('for', 'IN'), ('his', 'PRP$'), ('mercy', 'NN'), ('.', '.'), ('they', 'PRP'), ('told', 'VBD'), ('him', 'PRP'), ('that', 'IN'), ('they', 'PRP'), ('were', 'VBD'), ('experts', 'NNS'), ('at', 'IN'), ('burglary', 'NN'), ('and', 'CC'), ('could', 'MD'), ('help', 'VB'), ('the', 'DT'), ('king', 'NN'), ('in', 'IN'), ('catching', 'VBG'), ('other', 'JJ'), ('thieves.the', 'NN'), ('king', 'VBG'), ('being', 'VBG'), ('a', 'DT'), ('kind', 'NN'), ('ruler', 'NN'), ('asked', 'VBD'), ('his', 'PRP$'), ('guards', 'NNS'), ('to', 'TO'), ('release', 'VB'), ('them', 'PRP'), ('but', 'CC'), ('with', 'IN'), ('a', 'DT'), ('condition', 'NN'), ('.', '.'), ('he', 'PRP'), ('told', 'VBD'), ('the', 'DT'), ('burglars', 'NNS'), ('that', 'IN'), ('he', 'PRP'), ('would', 'MD'), ('release', 'VB'), ('them', 'PRP'), ('and', 'CC'), ('appoint', 'VB'), ('them', 'PRP'), ('as', 'IN'), ('his', 'PRP$'), ('spies', 'NNS'), ('only', 'RB'), ('if', 'IN'), ('they', 'PRP'), ('could', 'MD'), ('break', 'VB'), ('into', 'IN'), ('his', 'PRP$'), ('advisor', 'NN'), ('tenali', 'NN'), ('raman', 'NN'), ('’', 'NNP'), ('s', 'NN'), ('house', 'NN'), ('and', 'CC'), ('steal', 'JJ'), ('valuables', 'NNS'), ('from', 'IN'), ('there', 'RB'), ('.', '.'), ('the', 'DT'), ('thieves', 'NNS'), ('agreed', 'VBD'), ('for', 'IN'), ('the', 'DT'), ('challenge.that', 'NN'), ('same', 'JJ'), ('night', 'NN'), ('the', 'DT'), ('two', 'CD'), ('thieves', 'NNS'), ('went', 'VBD'), ('to', 'TO'), ('tenali', 'VB'), ('raman', 'JJ'), ('’', 'NNP'), ('s', 'NN'), ('house', 'NN'), ('and', 'CC'), ('hid', 'NN'), ('behind', 'IN'), ('some', 'DT'), ('bushes', 'NNS'), ('.', '.'), ('after', 'IN'), ('dinner', 'NN'), (',', ','), ('when', 'WRB'), ('tenali', 'NN'), ('raman', 'NN'), ('came', 'VBD'), ('out', 'RP'), ('for', 'IN'), ('a', 'DT'), ('stroll', 'NN'), (',', ','), ('he', 'PRP'), ('heard', 'VBD'), ('some', 'DT'), ('rustling', 'NN'), ('in', 'IN'), ('the', 'DT'), ('bushes', 'NNS'), ('.', '.'), ('he', 'PRP'), ('at', 'IN'), ('once', 'RB'), ('perceived', 'VBN'), ('the', 'DT'), ('existence', 'NN'), ('of', 'IN'), ('thieves', 'NNS'), ('in', 'IN'), ('his', 'PRP$'), ('garden', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "tagged_text=nltk.pos_tag(word_tokenize(text))\n",
    "print(tagged_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sundar', 'GPE'), ('CEO of Google', 'ORGANIZATION'), ('American', 'GPE')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "text3 = 'Sundar is the CEO of Google which is an American company.'\n",
    "tokenised = nltk.word_tokenize(text3)\n",
    "tagged_text3 = nltk.pos_tag(tokenised)\n",
    "ne_chunked = nltk.ne_chunk(tagged_text3)\n",
    "named_entities = []\n",
    "for tagged_tree in ne_chunked:\n",
    "    if hasattr(tagged_tree , 'label'):\n",
    "        entity_name =' '.join(c[0] for c in tagged_tree.leaves())\n",
    "        entity_type = tagged_tree.label()\n",
    "        named_entities.append((entity_name,entity_type))\n",
    "        \n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Sundar/NN)\n",
      "  is/VBZ\n",
      "  the/DT\n",
      "  (ORGANIZATION CEO/NN of/IN Google/NNP)\n",
      "  which/WDT\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  (GPE American/JJ)\n",
      "  company/NN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "print(ne_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
